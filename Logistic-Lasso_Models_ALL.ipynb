{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adverse-click",
   "metadata": {},
   "source": [
    "# Models by lapse month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-transition",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "federal-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import functions\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "##Import sklearn models & metrics\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression,RidgeClassifierCV, LassoCV, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score, plot_roc_curve, roc_curve, RocCurveDisplay, auc, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFE, SelectKBest, chi2, f_classif, SelectFromModel\n",
    "from sklearn.svm import SVR, SVC    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "planned-pleasure",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-541d978243fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Data cleaning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvar_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'var_details.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "## Data cleaning\n",
    "\n",
    "df = df1.copy(deep=False)\n",
    "\n",
    "var_details = pd.read_csv('var_details.csv')\n",
    "vars_cat = var_details.loc[(var_details.VarType==\"categorical\") & (var_details.Drop==\"No\")]\n",
    "vars_cat = vars_cat['Variable'].dropna()\n",
    "df = df.drop(['RMUSIC', 'RPHOTO', 'RVIDDVD', 'Count.of.Calls'], axis = 1)\n",
    "df_cols = df.columns\n",
    "vars_cat = [x for x in vars_cat if x in df_cols]\n",
    "df = pd.get_dummies(df, columns = vars_cat)\n",
    "df['Voluntary.Disenrollment'] = np.where((df['Voluntary.Disenrollment'] != \"No \"),1, 0)\n",
    "df = df.loc[((df['Voluntary.Disenrollment'] == 0) & (df.elapsedMonths>12)) | (df['Voluntary.Disenrollment']==1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create & summarize 3 month lapse model\n",
    "\n",
    "df_3month = df.copy(deep=False)\n",
    "df_3month = df_3month.loc[((df_3month.Lapsed==0) & (df_3month.elapsedMonths>3))| (df_3month.Lapsed==1)]\n",
    "df_3month.loc[df_3month.elapsedMonths<=3, ('EarlyLapser')] = 1\n",
    "df_3month.loc[df_3month.elapsedMonths>3, ('EarlyLapser')] = 0\n",
    "df_3month = df_3month.reset_index()\n",
    "\n",
    "print(df_3month.info())\n",
    "print(df_3month['EarlyLapser'].value_counts())\n",
    "print(df_3month['Lapsed'].value_counts())\n",
    "print(df_3month['Voluntary.Disenrollment'].value_counts())\n",
    "\n",
    "plt.scatter(df_3month['elapsedMonths'], df_3month['Lapsed'])\n",
    "plt.vlines(x=3, ymin=0, ymax=1)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df_3month['elapsedMonths'], df_3month['Voluntary.Disenrollment'])\n",
    "plt.vlines(x=12, ymin=0, ymax=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create & summarize 6 month lapse model\n",
    "\n",
    "df_6month = df.copy()\n",
    "df_6month = df_6month.loc[((df_6month.Lapsed==0) & (df_6month.elapsedMonths>6))| (df_6month.Lapsed==1)]\n",
    "df_6month.loc[df_6month.elapsedMonths<=6, ('EarlyLapser')] = 1\n",
    "df_6month.loc[df_6month.elapsedMonths>6, ('EarlyLapser')] = 0\n",
    "df_6month = df_6month.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "print(df_6month['EarlyLapser'].value_counts())\n",
    "print(df_6month['Lapsed'].value_counts())\n",
    "print(df_6month['Voluntary.Disenrollment'].value_counts())\n",
    "\n",
    "plt.scatter(df_6month['elapsedMonths'], df_6month['Lapsed'])\n",
    "plt.vlines(x=6, ymin=0, ymax=1)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df_6month['elapsedMonths'], df_6month['Voluntary.Disenrollment'])\n",
    "plt.vlines(x=12, ymin=0, ymax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create & summarize 1 year lapse model\n",
    "\n",
    "df_1year = df.copy()\n",
    "df_1year = df_1year.loc[((df_1year.Lapsed==0) & (df_1year.elapsedMonths>12))| (df_1year.Lapsed==1)]\n",
    "df_1year.loc[df_1year.elapsedMonths<=12, ('EarlyLapser')] = 1\n",
    "df_1year.loc[df_1year.elapsedMonths>12, ('EarlyLapser')] = 0\n",
    "df_1year = df_1year.reset_index()\n",
    "\n",
    "print(df_1year['EarlyLapser'].value_counts())\n",
    "print(df_1year['Lapsed'].value_counts())\n",
    "print(df_1year['Voluntary.Disenrollment'].value_counts())\n",
    "\n",
    "plt.scatter(df_1year['elapsedMonths'], df_1year['Lapsed'])\n",
    "plt.vlines(x=12, ymin=0, ymax=1)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df_1year['elapsedMonths'], df_1year['Voluntary.Disenrollment'])\n",
    "plt.vlines(x=12, ymin=0, ymax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define functions\n",
    "\n",
    "def categorical_evaluation(y_test, y_pred):\n",
    "    \n",
    "    \n",
    "    fpr, tpr, threshold = roc_curve(y_test, y_pred, pos_label = 1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"AUC:\", round(roc_auc, 2))\n",
    "    print(\"ROC Curve: \")\n",
    "    display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='Model')\n",
    "    display.plot()  \n",
    "    plt.show()\n",
    "    print(\"F1 Score: {}\", round(f1_score(y_test, y_pred), 4))\n",
    "    print(\"All Model Metrics: \\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Model Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "    return None\n",
    "   \n",
    "\n",
    "def continuous_evaluation(y_test, y_pred):\n",
    "    \n",
    "    fpr, tpr, threshold = roc_curve(y_test, y_pred, pos_label = 1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"AUC:\", round(roc_auc, 2))\n",
    "    print(\"ROC Curve: \")\n",
    "    display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='Model')\n",
    "    display.plot()  \n",
    "    plt.show()\n",
    "\n",
    "    thresh = np.linspace(0,1,1001)\n",
    "    f1_scores = []\n",
    "    for i in range(len(thresh)):\n",
    "        f1_scores.append(f1_score(y_test, classify(y_pred, thresh[i])))\n",
    "         \n",
    "    max_index = f1_scores.index(max(f1_scores))\n",
    "    thresholdOpt = thresh[max_index]   \n",
    "    print('Best Threshold: {}'.format(thresholdOpt))\n",
    "    print(\"F1 Score: \", f1_score(y_test, classify(y_pred, thresholdOpt)))        \n",
    "    print(\"All Model Metrics: \\n\", classification_report(y_test, classify(y_pred, thresholdOpt)))\n",
    "    print(\"Model Confusion Matrix: \\n\", confusion_matrix(y_test, classify(y_pred, thresholdOpt)))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def classify_decision_boundary(y_test, y_pred):\n",
    "    \n",
    "    thresh = np.linspace(0,1,1001)\n",
    "    f1_scores = []\n",
    "    for i in range(len(thresh)):\n",
    "        f1_scores.append(f1_score(y_test, classify(y_pred, thresh[i])))\n",
    "         \n",
    "    max_index = f1_scores.index(max(f1_scores))\n",
    "    thresholdOpt = thresh[max_index]\n",
    "    y = classify(y_pred, thresholdOpt)\n",
    "    return y\n",
    "\n",
    "def classify(y, x):\n",
    "    conditions = [(y >= x),\n",
    "             (y < x)]\n",
    "    values = [1.0, 0.0]\n",
    "    y = np.select(conditions, values)\n",
    "    return y\n",
    "\n",
    "def upsampler(x, y):\n",
    "    \n",
    "    x= pd.DataFrame(x)\n",
    "    training = pd.concat([x, y], axis = 1)\n",
    "    EarlyLapser = training[training.EarlyLapser==1]\n",
    "    Retained = training[training.EarlyLapser==0]\n",
    "              \n",
    "    if (len(EarlyLapser[0]) < len(Retained[0])):\n",
    "        \n",
    "        EarlyLapser_upsampled = resample(EarlyLapser,\n",
    "                                replace=True, # sample with replacement\n",
    "                                n_samples=len(Retained), # match number in majority class\n",
    "                                random_state=27, stratify = EarlyLapser) # reproducible results\n",
    "        upsampled = pd.concat([EarlyLapser_upsampled, Retained])\n",
    "    else:\n",
    "        Retained_upsampled = resample(Retained,\n",
    "                                replace=True, # sample with replacement\n",
    "                                n_samples=len(EarlyLapser), # match number in majority class\n",
    "                                random_state=27, stratify = Retained) # reproducible results\n",
    "        upsampled = pd.concat([EarlyLapser, Retained_upsampled])\n",
    "        \n",
    "    y_upsampled = upsampled['EarlyLapser']\n",
    "    x_upsampled = upsampled.drop('EarlyLapser', axis=1)\n",
    "    imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    imp_median.fit(x_upsampled)\n",
    "    x_upsampled = imp_median.transform(x_upsampled)\n",
    "    return(x_upsampled, y_upsampled)\n",
    "\n",
    "def downsampler(x, y):\n",
    "    \n",
    "    x= pd.DataFrame(x)\n",
    "    training = pd.concat([x, y], axis = 1)    \n",
    "    EarlyLapser = training[training.EarlyLapser==1]\n",
    "    Retained = training[training.EarlyLapser==0]\n",
    "    \n",
    "    if (len(EarlyLapser[0]) < len(Retained[0])):\n",
    "        Retained_downsampled = resample(Retained,\n",
    "                                replace=False, # sample without replacement\n",
    "                                n_samples=len(EarlyLapser), # match number in majority class\n",
    "                                random_state=27, stratify = Retained) # reproducible results\n",
    "        downsampled = pd.concat([EarlyLapser, Retained_downsampled])\n",
    "    else:\n",
    "        EarlyLapser_downsampled = resample(EarlyLapser,\n",
    "                                replace=False, # sample with replacement\n",
    "                                n_samples=len(Retained), # match number in majority class\n",
    "                                random_state=27, stratify = EarlyLapser) # reproducible results\n",
    "\n",
    "        downsampled = pd.concat([EarlyLapser_downsampled, Retained])\n",
    "        \n",
    "    y_downsampled = downsampled['EarlyLapser']\n",
    "    x_downsampled = downsampled.drop('EarlyLapser', axis=1)\n",
    "    imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    imp_median.fit(x_downsampled)\n",
    "    x_downsampled = imp_median.transform(x_downsampled)\n",
    "    return(x_downsampled, y_downsampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Create train test split\n",
    "\n",
    "#3 month\n",
    "y_3month = df_3month['EarlyLapser']\n",
    "x_3month = df_3month.drop(['EarlyLapser', 'elapsedMonths', 'Lapsed', 'Voluntary.Disenrollment'], axis = 1)\n",
    "\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp_median.fit(x_3month)\n",
    "x_3month = imp_median.transform(x_3month)\n",
    "\n",
    "x_train_3month, x_test_3month, y_train_3month, y_test_3month = train_test_split(x_3month, y_3month, test_size=0.2, random_state=0)\n",
    "\n",
    "#6 month\n",
    "\n",
    "y_6month = df_6month['EarlyLapser']\n",
    "x_6month = df_6month.drop(['EarlyLapser', 'elapsedMonths', 'Lapsed', 'Voluntary.Disenrollment'], axis = 1)\n",
    "\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp_median.fit(x_6month)\n",
    "x_6month = imp_median.transform(x_6month)\n",
    "\n",
    "x_train_6month, x_test_6month, y_train_6month, y_test_6month = train_test_split(x_6month, y_6month, test_size=0.2, random_state=0)\n",
    "\n",
    "y_1year = df_1year['EarlyLapser']\n",
    "x_1year = df_1year.drop(['EarlyLapser', 'elapsedMonths', 'Lapsed', 'Voluntary.Disenrollment'], axis = 1)\n",
    "\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp_median.fit(x_1year)\n",
    "x_1year = imp_median.transform(x_1year)\n",
    "\n",
    "x_train_1year, x_test_1year, y_train_1year, y_test_1year = train_test_split(x_1year, y_1year, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Create upsamples and downsamples\n",
    "x_3month_upsampled, y_3month_upsampled = upsampler(x_3month, y_3month)\n",
    "x_3month_downsampled, y_3month_downsampled = downsampler(x_3month, y_3month)\n",
    "x_train_3month_upsampled, x_test_3month_upsampled, y_train_3month_upsampled, y_test_3month_upsampled = train_test_split(x_3month_upsampled, y_3month_upsampled, test_size=0.2, random_state=0)\n",
    "x_train_3month_downsampled, x_test_3month_downsampled, y_train_3month_downsampled, y_test_3month_downsampled = train_test_split(x_3month_downsampled, y_3month_downsampled, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "x_6month_upsampled, y_6month_upsampled = upsampler(x_6month, y_6month)\n",
    "x_6month_downsampled, y_6month_downsampled = downsampler(x_6month, y_6month)\n",
    "x_train_6month_upsampled, x_test_6month_upsampled, y_train_6month_upsampled, y_test_6month_upsampled = train_test_split(x_6month_upsampled, y_6month_upsampled, test_size=0.2, random_state=0)\n",
    "x_train_6month_downsampled, x_test_6month_downsampled, y_train_6month_downsampled, y_test_6month_downsampled = train_test_split(x_6month_downsampled, y_6month_downsampled, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "x_1year_upsampled, y_1year_upsampled = upsampler(x_1year, y_1year)\n",
    "x_1year_downsampled, y_1year_downsampled = downsampler(x_1year, y_1year)\n",
    "x_train_1year_upsampled, x_test_1year_upsampled, y_train_1year_upsampled, y_test_1year_upsampled = train_test_split(x_1year_upsampled, y_1year_upsampled, test_size=0.2, random_state=0)\n",
    "x_train_1year_downsampled, x_test_1year_downsampled, y_train_1year_downsampled, y_test_1year_downsampled = train_test_split(x_1year_downsampled, y_1year_downsampled, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create validation set\n",
    "\n",
    "x_train_3month_val, x_test_3month_val, y_train_3month_val, y_test_3month_val = train_test_split(x_3month, y_3month, test_size=0.05, random_state=0)\n",
    "x_train_6month_val, x_test_6month_val, y_train_6month_val, y_test_6month_val = train_test_split(x_6month, y_6month, test_size=0.05, random_state=0)\n",
    "x_train_1year_val, x_test_1year_val, y_train_1year_val, y_test_1year_val = train_test_split(x_1year, y_1year, test_size=0.05, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-compression",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-domestic",
   "metadata": {},
   "source": [
    "## 3 month model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Logistic Regression Model w/ stochastic gradient descent\n",
    "\n",
    "clf_3month = make_pipeline(StandardScaler(), SGDClassifier(loss = 'log', warm_start = True, learning_rate = 'adaptive', eta0 = 0.1, max_iter = 10000, penalty = 'elasticnet', class_weight = 'balanced')).fit(x_train_3month, y_train_3month)\n",
    "y_pred_3month_log = clf_3month.predict(np.array(x_test_3month))\n",
    "\n",
    "scores = cross_val_score(clf_3month, x_test_3month_val, y_test_3month_val, cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (round(scores.mean(),2), round(scores.std(), 2)))\n",
    "\n",
    "categorical_evaluation(y_test_3month, y_pred_3month_log)\n",
    "\n",
    "print(\"UPSAMPLED: \")\n",
    "\n",
    "clf_3month_upsampled = make_pipeline(StandardScaler(), SGDClassifier(loss = 'log', warm_start = True, learning_rate = 'adaptive', eta0 = 0.1, max_iter = 10000, penalty = 'elasticnet', class_weight = 'balanced')).fit(x_train_3month_upsampled, y_train_3month_upsampled)\n",
    "y_pred_3month_log_upsampled = clf_3month_upsampled.predict(np.array(x_test_3month))\n",
    "\n",
    "scores = cross_val_score(clf_3month_upsampled, x_test_3month_val, y_test_3month_val, cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (round(scores.mean(),2), round(scores.std(), 2)))\n",
    "\n",
    "categorical_evaluation(y_test_3month, y_pred_3month_log_upsampled)\n",
    "\n",
    "print(\"DOWNSAMPLED: \")\n",
    "\n",
    "clf_3month_downsampled = make_pipeline(StandardScaler(), SGDClassifier(loss = 'log', warm_start = True, learning_rate = 'adaptive', eta0 = 0.1, max_iter = 10000, penalty = 'elasticnet', class_weight = 'balanced')).fit(x_train_3month_downsampled, y_train_3month_downsampled)\n",
    "y_pred_3month_log_downsampled = clf_3month_downsampled.predict(np.array(x_test_3month))\n",
    "\n",
    "scores = cross_val_score(clf_3month_downsampled, x_test_3month_val, y_test_3month_val, cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (round(scores.mean(),2), round(scores.std(), 2)))\n",
    "\n",
    "categorical_evaluation(y_test_3month, y_pred_3month_log_downsampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-greenhouse",
   "metadata": {},
   "source": [
    "## 6 month model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Logistic Regression Model w/ stochastic gradient descent\n",
    "\n",
    "clf_6month = make_pipeline(StandardScaler(), SGDClassifier(loss = 'log', warm_start = True, learning_rate = 'adaptive', eta0 = 0.1, max_iter = 10000, penalty = 'elasticnet', class_weight = 'balanced')).fit(x_train_6month, y_train_6month)\n",
    "y_pred_6month_log = clf_6month.predict(np.array(x_test_6month))\n",
    "\n",
    "scores = cross_val_score(clf_6month, x_test_6month_val, y_test_6month_val, cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (round(scores.mean(),2), round(scores.std(), 2)))\n",
    "\n",
    "categorical_evaluation(y_test_6month, y_pred_6month_log)\n",
    "\n",
    "print(\"UPSAMPLED: \")\n",
    "\n",
    "clf_6month_upsampled = make_pipeline(StandardScaler(), SGDClassifier(loss = 'log', warm_start = True, learning_rate = 'adaptive', eta0 = 0.1, max_iter = 10000, penalty = 'elasticnet', class_weight = 'balanced')).fit(x_train_6month_upsampled, y_train_6month_upsampled)\n",
    "y_pred_6month_log_upsampled = clf_6month_upsampled.predict(np.array(x_test_6month))\n",
    "\n",
    "scores = cross_val_score(clf_6month_upsampled, x_test_6month_val, y_test_6month_val, cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (round(scores.mean(),2), round(scores.std(), 2)))\n",
    "\n",
    "categorical_evaluation(y_test_6month, y_pred_6month_log_upsampled)\n",
    "\n",
    "print(\"DOWNSAMPLED: \")\n",
    "\n",
    "clf_6month_downsampled = make_pipeline(StandardScaler(), SGDClassifier(loss = 'log', warm_start = True, learning_rate = 'adaptive', eta0 = 0.1, max_iter = 10000, penalty = 'elasticnet', class_weight = 'balanced')).fit(x_train_6month_downsampled, y_train_6month_downsampled)\n",
    "y_pred_6month_log_downsampled = clf_6month_downsampled.predict(np.array(x_test_6month))\n",
    "\n",
    "scores = cross_val_score(clf_6month_downsampled, x_test_6month_val, y_test_6month_val, cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (round(scores.mean(),2), round(scores.std(), 2)))\n",
    "\n",
    "categorical_evaluation(y_test_6month, y_pred_6month_log_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-consistency",
   "metadata": {},
   "source": [
    "## 1 year model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Logistic Regression Model w/ stochastic gradient descent\n",
    "\n",
    "clf_1year = make_pipeline(StandardScaler(), SGDClassifier(loss = 'log', warm_start = True, learning_rate = 'adaptive', eta0 = 0.1, max_iter = 10000, penalty = 'elasticnet', class_weight = 'balanced')).fit(x_train_1year, y_train_1year)\n",
    "y_pred_1year_log = clf_1year.predict(np.array(x_test_1year))\n",
    "\n",
    "scores = cross_val_score(clf_1year, x_test_1year_val, y_test_1year_val, cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (round(scores.mean(),2), round(scores.std(), 2)))\n",
    "\n",
    "categorical_evaluation(y_test_1year, y_pred_1year_log)\n",
    "\n",
    "print(\"UPSAMPLED: \")\n",
    "\n",
    "clf_1year_upsampled = make_pipeline(StandardScaler(), SGDClassifier(loss = 'log', warm_start = True, learning_rate = 'adaptive', eta0 = 0.1, max_iter = 10000, penalty = 'elasticnet', class_weight = 'balanced')).fit(x_train_1year_upsampled, y_train_1year_upsampled)\n",
    "y_pred_1year_log_upsampled = clf_1year_upsampled.predict(np.array(x_test_1year))\n",
    "\n",
    "scores = cross_val_score(clf_1year_upsampled, x_test_1year_val, y_test_1year_val, cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (round(scores.mean(),2), round(scores.std(), 2)))\n",
    "\n",
    "categorical_evaluation(y_test_1year, y_pred_1year_log_upsampled)\n",
    "\n",
    "print(\"DOWNSAMPLED: \")\n",
    "\n",
    "clf_1year_downsampled = make_pipeline(StandardScaler(), SGDClassifier(loss = 'log', warm_start = True, learning_rate = 'adaptive', eta0 = 0.1, max_iter = 10000, penalty = 'elasticnet', class_weight = 'balanced')).fit(x_train_1year_downsampled, y_train_1year_downsampled)\n",
    "y_pred_1year_log_downsampled = clf_1year_downsampled.predict(np.array(x_test_1year))\n",
    "\n",
    "scores = cross_val_score(clf_1year_downsampled, x_test_1year_val, y_test_1year_val, cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (round(scores.mean(),2), round(scores.std(), 2)))\n",
    "\n",
    "categorical_evaluation(y_test_1year, y_pred_1year_log_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-candy",
   "metadata": {},
   "source": [
    "# Linear Regression w/ L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-williams",
   "metadata": {},
   "source": [
    "## 3 month model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Linear Regression Model w/ Lasso regularizer\n",
    "\n",
    "modellasso_3month = make_pipeline(StandardScaler(), make_pipeline(StandardScaler(), LassoCV(max_iter = 10000, selection = \"random\"))).fit(x_train_3month, y_train_3month)\n",
    "y_pred_3month_lasso = modellasso_3month.predict(np.array(x_test_3month))\n",
    "\n",
    "continuous_evaluation(y_test_3month, y_pred_3month_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Upsampled Linear Regression Model w/ Lasso regularizer\n",
    "\n",
    "modellasso_3month_upsampled = make_pipeline(StandardScaler(), LassoCV(max_iter = 10000, selection = \"random\")).fit(x_train_3month_upsampled, y_train_3month_upsampled)\n",
    "y_pred_3month_lasso_upsampled = modellasso_3month_upsampled.predict(np.array(x_test_3month))\n",
    "\n",
    "continuous_evaluation(y_test_3month, y_pred_3month_lasso_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Downsampled Linear Regression Model w/ Lasso regularizer\n",
    "\n",
    "modellasso_3month_downsampled =make_pipeline(StandardScaler(), LassoCV(max_iter = 10000, selection = \"random\")).fit(x_train_3month_downsampled, y_train_3month_downsampled)\n",
    "y_pred_3month_lasso_downsampled = modellasso_3month_downsampled.predict(np.array(x_test_3month))\n",
    "\n",
    "continuous_evaluation(y_test_3month, y_pred_3month_lasso_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-filename",
   "metadata": {},
   "source": [
    "## 6 month model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Linear Regression Model w/ Lasso regularizer\n",
    "\n",
    "modellasso_6month = make_pipeline(StandardScaler(), LassoCV(max_iter = 10000, selection = \"random\")).fit(x_train_6month, y_train_6month)\n",
    "y_pred_6month_lasso = modellasso_6month.predict(np.array(x_test_6month))\n",
    "\n",
    "continuous_evaluation(y_test_6month, y_pred_6month_lasso)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Upsampled Linear Regression Model w/ Lasso regularizer\n",
    "\n",
    "modellasso_6month_upsampled = make_pipeline(StandardScaler(), LassoCV(max_iter = 10000, selection = \"random\")).fit(x_train_6month_upsampled, y_train_6month_upsampled)\n",
    "y_pred_6month_lasso_upsampled = modellasso_6month_upsampled.predict(np.array(x_test_6month))\n",
    "\n",
    "continuous_evaluation(y_test_6month, y_pred_6month_lasso_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Downsampled Linear Regression Model w/ Lasso regularizer\n",
    "\n",
    "modellasso_6month_downsampled = make_pipeline(StandardScaler(), LassoCV(max_iter = 10000, selection = \"random\")).fit(x_train_6month_downsampled, y_train_6month_downsampled)\n",
    "y_pred_6month_lasso_downsampled = modellasso_6month_downsampled.predict(np.array(x_test_6month))\n",
    "\n",
    "continuous_evaluation(y_test_6month, y_pred_6month_lasso_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-healthcare",
   "metadata": {},
   "source": [
    "## 1 year model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Linear Regression Model w/ Lasso regularizer\n",
    "\n",
    "modellasso_1year = make_pipeline(StandardScaler(), LassoCV(max_iter = 10000, random_state = 6)).fit(x_train_1year, y_train_1year)\n",
    "y_pred_1year_lasso = modellasso_1year.predict(np.array(x_test_1year))\n",
    "\n",
    "continuous_evaluation(y_test_1year, y_pred_1year_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Upsampled Linear Regression Model w/ Lasso regularizer\n",
    "\n",
    "modellasso_1year_upsampled = make_pipeline(StandardScaler(), LassoCV(max_iter = 100000, random_state=6)).fit(x_train_1year_upsampled, y_train_1year_upsampled)\n",
    "y_pred_1year_lasso_upsampled = modellasso_1year_upsampled.predict(np.array(x_test_1year))\n",
    "\n",
    "continuous_evaluation(y_test_1year, y_pred_1year_lasso_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Downsampled Linear Regression Model w/ Lasso regularizer\n",
    "\n",
    "modellasso_1year_downsampled = make_pipeline(StandardScaler(), LassoCV(max_iter = 10000, selection = \"random\")).fit(x_train_1year_downsampled, y_train_1year_downsampled)\n",
    "y_pred_1year_lasso_downsampled = modellasso_1year_downsampled.predict(np.array(x_test_1year))\n",
    "\n",
    "continuous_evaluation(y_test_1year, y_pred_1year_lasso_downsampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
